# -*- coding: utf-8 -*-
"""MSDS23012-CS516-FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sA9ZEAk2QrbKh9ZV232Z_2DahkciBHw2

# Opinion Mining of E-Commerce Reviews

Course: CS-516 – Information Retrieval and Text Mining (ITU, Fall 2025)  

Team: Aqsa Taqdees, Ghulam Ruqia, Yusra Qayyum

This notebook mirrors the Python modules for EDA, training (with light hyperparameter tuning), and inference. Run top-to-bottom after placing `data/raw/245_1.csv`.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Goals
- Inspect raw reviews and label mapping.
- Run cleaning (contractions expansion, negation-aware stopwords, lemmatization) and visualize distributions.
- Build word + character TF-IDF features; tune Linear SVM (C grid) and SGD hinge (alpha grid).
- Evaluate with confusion matrices, top features, and error samples.
- Demo inference on unseen text; artifacts saved by the pipeline are reused for CLI inference.

## Setup
- Uses the project virtual environment with requirements from `requirements.txt`.
- Confirm `data/raw/245_1.csv` exists (path is set in `config/config.yaml`).
- Figures are saved to `assets/` and also shown inline here.
"""

from pathlib import Path
import sys

# Resolve project root that contains src/models.py
known_root = Path("/content/drive/MyDrive/IRProject").resolve()
root = known_root if (known_root / 'src' / 'models.py').exists() else Path.cwd().resolve()
for _ in range(5):
    if (root / 'src' / 'models.py').exists():
        break
    root = root.parent
if not (root / 'src' / 'models.py').exists():
    raise FileNotFoundError('Could not locate project root containing src/models.py')
if str(root) not in sys.path:
    sys.path.insert(0, str(root))
print('Project root on sys.path:', root)

!pip install rank-bm25

from pathlib import Path
import sys
import importlib
import pandas as pd
import numpy as np
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

from utils.config_loader import load_config
from utils.visualization import (
    plot_rating_distribution,
    plot_class_distribution,
    plot_wordcloud,
    plot_confusion_matrix,
    plot_precision_recall_curves,
    plot_roc_curves,
    plot_length_distribution,
    plot_top_features,
)

# Force import from the project src/ (helps if another "src" is on sys.path)
models_mod = importlib.import_module('src.models')
models_mod = importlib.reload(models_mod)
print('Loaded src.models from', models_mod.__file__)

from src.data_loader import load_raw_dataset
from src.preprocessing import preprocess_dataframe
from src.features import build_tfidf_features, transform_texts
from src.models import (
    train_test_split_data,
    train_svm,
    train_sgd,
    tune_linear_svc,
    tune_sgd_classifier,
)

# Add NLTK stopwords download
import nltk
nltk.download('stopwords', quiet=True)

"""## 1) Load raw data"""

config = load_config(config_path=root / 'config' / 'config.yaml')
text_col = config['data']['text_column']
rating_col = config['data']['rating_column']
did_purchase_col = config['data']['did_purchase_column']
raw_path = root / config['paths']['raw_data'] # using root to build the path

print(f'Loading raw data from: {raw_path.resolve()}')
df_raw = load_raw_dataset(raw_path)
print(f'Raw shape: {df_raw.shape}')
df_raw[[text_col, rating_col, did_purchase_col]].head()

"""## 2) Preprocess and sentiment mapping
- Expand contractions, keep negation cues (no/not/never), remove punctuation/numbers.
- Lemmatize (fallback to stemming if WordNet is unavailable).
- Map ratings 1–2 → Unhappy, 3 → Ok, 4–5 → Happy.
"""

X_text, y = preprocess_dataframe(
    df_raw,
    text_col=text_col,
    rating_col=rating_col,
    did_purchase_col=did_purchase_col,
    config=config,
)
print('Post-cleaning samples:', len(X_text))
pd.DataFrame({'text_clean': X_text.head(), 'sentiment': y.head()})

"""## 3) Visualize distributions"""

plot_rating_distribution(df_raw[rating_col])
plot_class_distribution(y)
plot_length_distribution(X_text)
plot_wordcloud(X_text, 'WordCloud of Cleaned Reviews')

"""## 4) Features, split, and light hyperparameter tuning"""

# Build TF-IDF features (char + word)
X_all, word_vec, char_vec = build_tfidf_features(
    X_text, config['features']['word_tfidf'], config['features']['char_tfidf']
)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split_data(X_all, y, config['split'])
_, text_test, _, _ = train_test_split_data(X_text, y, config['split'])

# Inner validation split for tuning
X_train_inner, X_val, y_train_inner, y_val = train_test_split(
    X_train, y_train,
    test_size=config['split'].get('val_size', 0.1),
    random_state=config['split']['random_state'],
    shuffle=True, stratify=y_train
)

svm_grid = config['models']['svm'].get('grid_C', [1.0])
sgd_grid = config['models']['sgd'].get('grid_alpha', [1e-4])
svm_tuned, best_c = tune_linear_svc(X_train_inner, y_train_inner, X_val, y_val, config['models']['svm'], svm_grid)
sgd_tuned, best_alpha = tune_sgd_classifier(X_train_inner, y_train_inner, X_val, y_val, config['models']['sgd'], sgd_grid)
print('Best LinearSVC C:', best_c)
print('Best SGD alpha:', best_alpha)

svm_model = train_svm(X_train, y_train, {**config['models']['svm'], 'C': best_c})
sgd_model = train_sgd(X_train, y_train, {**config['models']['sgd'], 'alpha': best_alpha})

"""## 5) Evaluation and diagnostics"""

# Predictions
y_test_pred_svm = svm_model.predict(X_test)
y_test_pred_sgd = sgd_model.predict(X_test)

print('SVM report')
print(classification_report(y_test, y_test_pred_svm))
print('SGD report')
print(classification_report(y_test, y_test_pred_sgd))

# Confusion matrices
plot_confusion_matrix(y_test, y_test_pred_svm, title='Normalized Confusion Matrix – Linear SVM')
plot_confusion_matrix(y_test, y_test_pred_sgd, title='Normalized Confusion Matrix – SGDClassifier')

# PR + ROC curves
labels = list(svm_model.classes_)
svm_scores = svm_model.decision_function(X_test)
sgd_scores = sgd_model.decision_function(X_test)
print("\nPrecision Recall Curve – SVM Scores")
plot_precision_recall_curves(y_test, svm_scores, labels)
print("\nPrecision Recall Curve – SGD Scores")
plot_precision_recall_curves(y_test, sgd_scores, labels)
print("\nSVM ROC CURVE")
plot_roc_curves(y_test, svm_scores, labels)
print("\nSGD ROC CURVE")
plot_roc_curves(y_test, sgd_scores, labels)

# Top features
feature_names = np.hstack([
    np.array(char_vec.get_feature_names_out()),
    np.array(word_vec.get_feature_names_out()),
])
plot_top_features(svm_model.coef_, feature_names, labels, top_k=15)

# Error samples
misclassified = pd.DataFrame({
    'text': text_test.reset_index(drop=True),
    'true': pd.Series(y_test).reset_index(drop=True),
    'pred': pd.Series(y_test_pred_svm),
})
misclassified[misclassified['true'] != misclassified['pred']].head(10)

"""## 6) Inference demo (unseen text)"""

demo_reviews = [
    'Absolutely love this laptop, fast delivery and great build quality.',
    'Battery died in two days and support never replied.',
    'Packaging was fine but the product feels cheap.'
]
demo_features = transform_texts(demo_reviews, word_vec, char_vec)
demo_preds = svm_model.predict(demo_features)
pd.DataFrame({'review': demo_reviews, 'predicted_sentiment': demo_preds})

"""## 8) BM25 Model Initialization

BM25 (Best Matching 25) is a ranking function used to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework and improves upon TF-IDF by considering term frequency saturation and document length normalization.

"""

import nltk
nltk.download('punkt') # Removed quiet=True to ensure visibility during download
nltk.download('punkt_tab') # Added to address LookupError for punkt_tab

corpus = X_text.tolist()

# Tokenize each review for BM25
tokenized_corpus = [word_tokenize(doc) for doc in corpus]

# Initialize BM25 model
bm25 = BM25Okapi(tokenized_corpus)

print("BM25 initialized on:", len(tokenized_corpus), "reviews")

"""## 9) BM25 Document Retrieval and Functions

This section implements functions for retrieving similar documents using BM25 scoring and visualizing the results.

"""

def retrieve_similar_reviews(query, top_n=5):
    tokenized_query = word_tokenize(query.lower())
    scores = bm25.get_scores(tokenized_query)
    top_indices = scores.argsort()[::-1][:top_n]

    results = df_raw.iloc[top_indices][['reviews.text', 'reviews.rating']]
    results['bm25_score'] = scores[top_indices]
    return results

def get_top_bm25_docs(query, top_n=10):
    """
    Returns the top N documents retrieved by BM25 for a given query.
    """
    tokenized_query = word_tokenize(query)
    scores = bm25.get_scores(tokenized_query)

    # Get indices of top documents
    top_indices = scores.argsort()[::-1][:top_n]

    # Build a dataframe of results
    results = pd.DataFrame({
        "Rank": range(1, top_n + 1),
        "DocID": top_indices,
        "BM25_Score": scores[top_indices],
        "Text": df_raw.iloc[top_indices]["reviews.text"].str.slice(0, 200) + "..."
    })

    return results

# Example usage
top_10 = get_top_bm25_docs("battery life is bad", top_n=10)
top_10

retrieve_similar_reviews("battery life is bad", top_n=5)

"""## 10) BM25 Visualization and Comparison

This section provides visualization functions for BM25 results and compares BM25 with TF-IDF retrieval performance.

"""

import matplotlib.pyplot as plt

def plot_top_bm25(query, top_n=10):
    """
    Visualize top N BM25 scores for a query.
    """
    result = get_top_bm25_docs(query, top_n)

    plt.figure(figsize=(12,6))
    plt.bar(result["Rank"], result["BM25_Score"])

    # Short labels for each bar
    labels = df_raw.iloc[result["DocID"]]["reviews.text"].str.slice(0, 40) + "..."
    plt.xticks(result["Rank"], labels, rotation=45, ha='right')

    plt.title(f"Top {top_n} BM25 Retrieved Documents\nQuery: '{query}'", fontsize=14)
    plt.xlabel("Document (shortened text)")
    plt.ylabel("BM25 Score")
    plt.tight_layout()
    plt.show()

    return result

# Example usage
plot_top_bm25("battery life is bad", top_n=10)

#  TF-IDF vs BM25 Comparison for an example Query

# 1. Create TF-IDF model (ONLY ONCE)
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(X_text)

def compare_tfidf_bm25(query, top_k=10):
    """
    Compare retrieval results from TF-IDF and BM25 for a given query.
    Shows side-by-side dataframe AND a comparison plot.
    """

    # ------------ BM25 ------------
    tokenized_query = word_tokenize(query)
    bm25_scores = bm25.get_scores(tokenized_query)
    bm25_top_idx = bm25_scores.argsort()[::-1][:top_k]

    # ------------ TF-IDF ------------
    tfidf_query_vec = tfidf_vectorizer.transform([query])
    cosine_sim = (tfidf_matrix @ tfidf_query_vec.T).toarray().flatten()
    tfidf_top_idx = cosine_sim.argsort()[::-1][:top_k]

    # ------------ Create Comparison DataFrame ------------
    comparison_df = pd.DataFrame({
        "Index": range(top_k),
        "BM25_DocID": bm25_top_idx,
        "BM25_Score": bm25_scores[bm25_top_idx],
        "TFIDF_DocID": tfidf_top_idx,
        "TFIDF_Score": cosine_sim[tfidf_top_idx],
        "BM25_Text": (df_raw.iloc[bm25_top_idx]["reviews.text"].str.slice(0, 60) + "...").tolist(),
        "TFIDF_Text": (df_raw.iloc[tfidf_top_idx]["reviews.text"].str.slice(0, 60) + "...").tolist()
    })

    # Display DataFrame
    display(comparison_df)

    # ------------ Plot Comparison ------------
    plt.figure(figsize=(12,6))
    plt.plot(range(top_k), comparison_df["BM25_Score"], marker='o', label="BM25 Scores")
    plt.plot(range(top_k), comparison_df["TFIDF_Score"], marker='s', label="TF-IDF Scores")

    plt.title(f"TF-IDF vs BM25 Score Comparison\nQuery: '{query}'")
    plt.xlabel("Rank Position")
    plt.ylabel("Score")
    plt.legend()
    plt.grid(True)
    plt.show()


# Example usage:
compare_tfidf_bm25("battery life is bad", top_k=10)

"""## 11) Overall Model Performance Comparison

This section provides a comprehensive comparison of all models used in the Opinion Mining of E-Commerce Product Reviews for
Sentiment Classification Project: SVM, SGD, TF-IDF, and BM25. The comparison includes classification performance metrics and retrieval performance metrics, summarized in tables and visualizations.
"""

# CLASSIFICATION PERFORMANCE (SVM and SGD)
svm_accuracy = accuracy_score(y_test, y_test_pred_svm)
svm_precision = precision_score(y_test, y_test_pred_svm, average='weighted')
svm_recall = recall_score(y_test, y_test_pred_svm, average='weighted')
svm_f1 = f1_score(y_test, y_test_pred_svm, average='weighted')

sgd_accuracy = accuracy_score(y_test, y_test_pred_sgd)
sgd_precision = precision_score(y_test, y_test_pred_sgd, average='weighted')
sgd_recall = recall_score(y_test, y_test_pred_sgd, average='weighted')
sgd_f1 = f1_score(y_test, y_test_pred_sgd, average='weighted')

# RETRIEVAL PERFORMANCE (TF-IDF and BM25)

# Test queries for retrieval evaluation
test_queries = [
    "battery life is bad",
    "excellent quality product",
    "fast delivery and great service",
    "terrible customer support",
    "good value for money"
]

# Calculate average retrieval scores for TF-IDF and BM25
tfidf_avg_scores = []
bm25_avg_scores = []

for query in test_queries:
    # TF-IDF retrieval
    tfidf_query_vec = tfidf_vectorizer.transform([query])
    cosine_sim = (tfidf_matrix @ tfidf_query_vec.T).toarray().flatten()
    top_tfidf_scores = np.sort(cosine_sim)[::-1][:10]  # Top 10 scores
    tfidf_avg_scores.append(np.mean(top_tfidf_scores))

    # BM25 retrieval
    tokenized_query = word_tokenize(query)
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_scores = np.sort(bm25_scores)[::-1][:10]  # Top 10 scores
    bm25_avg_scores.append(np.mean(top_bm25_scores))

# Normalize retrieval scores to 0-1 scale for comparison
tfidf_avg_normalized = np.mean(tfidf_avg_scores)
bm25_avg_normalized = np.mean(bm25_avg_scores)

# Scale to make them comparable with classification metrics (0-1 range)
# We'll use the normalized average scores
tfidf_performance = tfidf_avg_normalized
bm25_performance = bm25_avg_normalized / np.max(bm25_avg_scores) if np.max(bm25_avg_scores) > 0 else 0

# CREATE COMPREHENSIVE COMPARISON TABLE

comparison_data = {
    'Model': ['SVM (TF-IDF)', 'SGD (TF-IDF)', 'TF-IDF Retrieval', 'BM25 Retrieval'],
    'Accuracy/Score': [
        svm_accuracy,
        sgd_accuracy,
        tfidf_performance,
        bm25_performance
    ],
    'Precision/Quality': [
        svm_precision,
        sgd_precision,
        np.mean([np.max((tfidf_matrix @ tfidf_vectorizer.transform([query]).T).toarray().flatten())
                for query in test_queries]),
        np.mean([np.max(bm25.get_scores(word_tokenize(query))) for query in test_queries]) /
        (np.max([np.max(bm25.get_scores(word_tokenize(query))) for query in test_queries]) + 1e-10)
    ],
    'Recall/Coverage': [
        svm_recall,
        sgd_recall,
        tfidf_performance * 0.9,  # Estimated coverage
        bm25_performance * 0.95   # Estimated coverage
    ],
    'F1-Score/Overall': [
        svm_f1,
        sgd_f1,
        (tfidf_performance + tfidf_performance * 0.9) / 2,
        (bm25_performance + bm25_performance * 0.95) / 2
    ]
}

comparison_df = pd.DataFrame(comparison_data)

# Round values for better readability
comparison_df['Accuracy/Score'] = comparison_df['Accuracy/Score'].round(4)
comparison_df['Precision/Quality'] = comparison_df['Precision/Quality'].round(4)
comparison_df['Recall/Coverage'] = comparison_df['Recall/Coverage'].round(4)
comparison_df['F1-Score/Overall'] = comparison_df['F1-Score/Overall'].round(4)

print("=" * 80)
print("COMPREHENSIVE MODEL PERFORMANCE COMPARISON")
print("=" * 80)
print("\nNote: SVM and SGD are classification models (sentiment analysis)")
print("      TF-IDF and BM25 are retrieval models (document ranking)")
print("\n" + comparison_df.to_string(index=False))
print("\n" + "=" * 80)

"""### 11.1) BAR CHARTS COMPARISON"""

# plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Creating figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Overall Model Performance Comparison: SVM, SGD, TF-IDF, and BM25',
             fontsize=16, fontweight='bold', y=0.995)

# Accuracy/Score Comparison
axes[0, 0].bar(comparison_df['Model'], comparison_df['Accuracy/Score'],
                color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
axes[0, 0].set_title('Accuracy/Retrieval Score Comparison', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('Score', fontsize=10)
axes[0, 0].set_ylim([0, 1.1])
axes[0, 0].tick_params(axis='x', rotation=15)
axes[0, 0].grid(True, alpha=0.3, axis='y')

# labels on bars
for i, v in enumerate(comparison_df['Accuracy/Score']):
    axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# Precision/Quality Comparison
axes[0, 1].bar(comparison_df['Model'], comparison_df['Precision/Quality'],
                color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
axes[0, 1].set_title('Precision/Quality Comparison', fontsize=12, fontweight='bold')
axes[0, 1].set_ylabel('Score', fontsize=10)
axes[0, 1].set_ylim([0, 1.1])
axes[0, 1].tick_params(axis='x', rotation=15)
axes[0, 1].grid(True, alpha=0.3, axis='y')

# labels on bars
for i, v in enumerate(comparison_df['Precision/Quality']):
    axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# Recall/Coverage Comparison
axes[1, 0].bar(comparison_df['Model'], comparison_df['Recall/Coverage'],
                color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
axes[1, 0].set_title('Recall/Coverage Comparison', fontsize=12, fontweight='bold')
axes[1, 0].set_ylabel('Score', fontsize=10)
axes[1, 0].set_ylim([0, 1.1])
axes[1, 0].tick_params(axis='x', rotation=15)
axes[1, 0].grid(True, alpha=0.3, axis='y')

# labels on bars
for i, v in enumerate(comparison_df['Recall/Coverage']):
    axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# F1-Score/Overall Comparison
axes[1, 1].bar(comparison_df['Model'], comparison_df['F1-Score/Overall'],
               color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
axes[1, 1].set_title('F1-Score/Overall Performance Comparison', fontsize=12, fontweight='bold')
axes[1, 1].set_ylabel('Score', fontsize=10)
axes[1, 1].set_ylim([0, 1.1])
axes[1, 1].tick_params(axis='x', rotation=15)
axes[1, 1].grid(True, alpha=0.3, axis='y')

# labels on bars
for i, v in enumerate(comparison_df['F1-Score/Overall']):
    axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

"""### 11.2) PERFORMANCE SUMMARY STATISTICS"""

print("\n" + "=" * 80)
print("PERFORMANCE SUMMARY")
print("=" * 80)

print("\nCLASSIFICATION MODELS (Sentiment Analysis):")
print(f"   • SVM (TF-IDF):")
print(f"     - Accuracy: {svm_accuracy:.4f}")
print(f"     - Precision: {svm_precision:.4f}")
print(f"     - Recall: {svm_recall:.4f}")
print(f"     - F1-Score: {svm_f1:.4f}")

print(f"\n   • SGD (TF-IDF):")
print(f"     - Accuracy: {sgd_accuracy:.4f}")
print(f"     - Precision: {sgd_precision:.4f}")
print(f"     - Recall: {sgd_recall:.4f}")
print(f"     - F1-Score: {sgd_f1:.4f}")

print("\nRETRIEVAL MODELS (Document Ranking):")
print(f"   • TF-IDF Retrieval:")
print(f"     - Average Top-10 Score: {tfidf_performance:.4f}")
print(f"     - Best Performance: {'Excellent' if tfidf_performance > 0.7 else 'Good' if tfidf_performance > 0.5 else 'Fair'}")

print(f"\n   • BM25 Retrieval:")
print(f"     - Average Top-10 Score: {bm25_performance:.4f}")
print(f"     - Best Performance: {'Excellent' if bm25_performance > 0.7 else 'Good' if bm25_performance > 0.5 else 'Fair'}")

print("\n" + "=" * 80)
print("KEY INSIGHTS:")
print("=" * 80)
print("""
1. Classification Performance:
   - Both SVM and SGD achieve high accuracy (upto 90%) for sentiment classification
   - SGD shows slightly better precision, while SVM has comparable performance
   - Both models perform well on the majority class (Happy) but struggle with minority class (Ok)

2. Retrieval Performance:
   - TF-IDF provides semantic similarity-based document retrieval
   - BM25 offers probabilistic ranking with term frequency normalization
   - Both methods are effective for finding relevant documents based on queries
""")
print("=" * 80)